{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263949b1-a28c-4e16-b4ed-77140f645663",
   "metadata": {},
   "source": [
    "# Applying functions to data files in batch\n",
    "\n",
    "This package contains functionality for defining a Python function and applying it to remote files in batches using the `gsbatch_apply` class. This notebook gives an example of how to use this class.\n",
    "\n",
    "In the getting_started notebook, we looked at creating batch object. These objects describe a list of files (local or remote on google buckets), separating them into batches and then downloading or uploading the data. Here, we extend that to show how we can easily perform a workflow with the following steps:\n",
    "\n",
    "    1. Download data from a google bucket in batches.\n",
    "    2. Apply a function to this batch of data one file at a time.\n",
    "    3. Save the analysed data to new files.\n",
    "    4. Push the resulting batch of new files back to the google bucket.\n",
    "    \n",
    "The `gsbatch_apply` object works by performing the following loop on any batch objects that are passed:\n",
    "\n",
    "1. Download data using `gsutil -m cp` from any batch objects where `source='remote'`. Data is downloaded to directory argument `get_dir` (required). \n",
    "2. Pass files from the current batch of all objects to a user provided function in the same order that batches were passed to gsbatch_apply.\n",
    "3. Function can be applied to batch files one at a time (`pass_args='one'`) or all at once (`pass_args = 'all'`).\n",
    "4. Concatenate any output from all function applications in the batch.\n",
    "5. Upload data using `gsutil -m cp` to remote directory `put_dir` (required if there are objects with `source='local'`). \n",
    "6. Delete temporary downloaded files from this batch.\n",
    "7. If `delete_put_files == True` then delete any batch files described by any objects where `source = 'local'`.\n",
    "8. Cycle all objects up to the next batch.\n",
    "9. Go back to step one until all batches are cycled.\n",
    "\n",
    "So let's start by importing things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2aec8784-dea1-4932-9d2d-7ba7d49f1b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gsbatch import Gsbatch, gsbatch_apply\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75866028-b6c6-4e91-a754-fe76e6b725ca",
   "metadata": {},
   "source": [
    "Next, we're going to create two Gsbatch objects. The first describes the data in the google bucket. The second describes local data that does not exist yet (but will once we have analysed bucket data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ce86842-93d6-470e-94c0-3fbb14604bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch object describing things on the cloud\n",
    "gsb_get = Gsbatch(gs_dir = \"gs://fqqzlp/test_data\",\n",
    "                  gs_list = '*.nc',\n",
    "                  source='remote',\n",
    "                  batch_size=4,\n",
    "                  get_dir = \"/home/davidbyrne/disks/ssd500/test\")\n",
    "\n",
    "# Create local batch object with same number of files and batch size\n",
    "# as the remote batch object. These will be names test0.nc, ..., testN.nc\n",
    "output_filenames = [f\"test{ii}.nc\" for ii in range(gsb_get.n_files)]\n",
    "gsb_put = Gsbatch(gs_dir = \"/home/davidbyrne/disks/ssd500/test\",\n",
    "                  gs_list = output_filenames,\n",
    "                  source='local',\n",
    "                  batch_size=4,\n",
    "                  put_dir = \"gs://fqqzlp/test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90210ff1-36df-436a-99c7-eb54bdc99bf4",
   "metadata": {},
   "source": [
    "Next we need to define a function to apply. Here we are going to take two file names `fp_in` and `fp_out`. We will open `fp_in` and take the mean of the data along a single axis (`'x'`) and save it to `fp_out`. We're using `xarray` to do this here, so that it can be done over multiple cores in parallel.\n",
    "\n",
    "We also return an integer (1) when the routine has been successful just to show how gsbatch_apply will concatenate outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0bd32733-077f-422d-9f21-3470553503f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_func(fp_in, fp_out):\n",
    "    ds = xr.open_dataset(fp_in, chunks={'lat':1e3, 'lon':1e3})\n",
    "    data_out = ds.mean(dim='lat')\n",
    "    data_out.to_netcdf(fp_out)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7177b3-bc0a-4b5f-b1fc-3747f3902137",
   "metadata": {},
   "source": [
    "Now we call `gsbatch_apply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e92225c2-069a-4104-9411-36ab135ffb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Applying function batch_func to 2 gsbatch object.\n",
      "\n",
      "   --> Resetting all batches\n",
      "   --> Number of batches: 3\n",
      "   --> Processing batch: 1 / 3\n",
      "      --> Getting data from 1 gsbatch objects.\n",
      "      --> Applying function one file at a time.\n",
      "      --> Uploading data to 1 gsbatch objects.\n",
      "   --> Processing batch: 2 / 3\n",
      "      --> Getting data from 1 gsbatch objects.\n",
      "      --> Applying function one file at a time.\n",
      "      --> Uploading data to 1 gsbatch objects.\n",
      "   --> Processing batch: 3 / 3\n",
      "      --> Getting data from 1 gsbatch objects.\n",
      "      --> Applying function one file at a time.\n",
      "      --> Uploading data to 1 gsbatch objects.\n",
      "Final batch has been reached. Cannot increase index.\n",
      "Final batch has been reached. Cannot increase index.\n",
      "Done! Phew.\n",
      "CPU times: user 51.5 s, sys: 1min 5s, total: 1min 56s\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "app = gsbatch_apply(func = batch_func, \n",
    "                    batch = [gsb_get, gsb_put],\n",
    "                    verbosity = 2,\n",
    "                    pass_args = 'one',\n",
    "                    delete_put_files = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "795c7e50-8820-4c7a-8bdb-0b2e54d048f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
