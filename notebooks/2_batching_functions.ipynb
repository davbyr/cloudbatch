{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263949b1-a28c-4e16-b4ed-77140f645663",
   "metadata": {},
   "source": [
    "# Applying functions to data files in batch\n",
    "\n",
    "This package contains functionality for defining a Python function and applying it to remote files in batches using the `CloudBatch_apply` class. This notebook gives an example of how to use this class.\n",
    "\n",
    "In the getting_started notebook, we looked at creating batch object. These objects describe a list of files (local or remote on google buckets), separating them into batches and then downloading or uploading the data. Here, we extend that to show how we can easily perform a workflow with the following steps:\n",
    "\n",
    "    1. Download data from a google bucket in batches.\n",
    "    2. Apply a function to this batch of data one file at a time.\n",
    "    3. Save the analysed data to new files.\n",
    "    4. Push the resulting batch of new files back to the google bucket.\n",
    "    \n",
    "The `CloudBatch_apply` object works by performing the following loop on any batch objects that are passed:\n",
    "\n",
    "1. Download data using `gsutil -m cp` from any batch objects where `source='remote'`. Data is downloaded to directory argument `get_dir` (required). \n",
    "2. Pass files from the current batch of all objects to a user provided function in the same order that batches were passed to CloudBatch_apply.\n",
    "3. Function can be applied to batch files one at a time (`pass_args='one'`) or all at once (`pass_args = 'all'`).\n",
    "4. Concatenate any output from all function applications in the batch.\n",
    "5. Upload data using `gsutil -m cp` to remote directory `put_dir` (required if there are objects with `source='local'`). \n",
    "6. Delete temporary downloaded files from this batch.\n",
    "7. If `delete_put_files == True` then delete any batch files described by any objects where `source = 'local'`.\n",
    "8. Cycle all objects up to the next batch.\n",
    "9. Go back to step one until all batches are cycled.\n",
    "\n",
    "So let's start by importing things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aec8784-dea1-4932-9d2d-7ba7d49f1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cloudbatch import CloudBatch, CloudBatch_apply\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75866028-b6c6-4e91-a754-fe76e6b725ca",
   "metadata": {},
   "source": [
    "Next, we're going to create two CloudBatch objects. The first describes the data in the google bucket. The second describes local data that does not exist yet (but will once we have analysed bucket data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce86842-93d6-470e-94c0-3fbb14604bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch object describing things on the cloud\n",
    "gsb_get = CloudBatch(file_dir = \"gs://<bucket>/<dir>\",\n",
    "                  file_list = '*.nc',\n",
    "                  source='remote',\n",
    "                  batch_size=4,\n",
    "                  get_dir = \"<local directory>\")\n",
    "\n",
    "# Create local batch object with same number of files and batch size\n",
    "# as the remote batch object. These will be names test0.nc, ..., testN.nc\n",
    "output_filenames = [f\"test{ii}.nc\" for ii in range(gsb_get.n_files)]\n",
    "gsb_put = CloudBatch(file_dir = \"<local directory>\",\n",
    "                  file_list = \"<anticipated filenames>\",\n",
    "                  source='local',\n",
    "                  batch_size=4,\n",
    "                  put_dir = \"gs://<bucket>/<dir>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90210ff1-36df-436a-99c7-eb54bdc99bf4",
   "metadata": {},
   "source": [
    "Next we need to define a function to apply. Here we are going to take two file names `fp_in` and `fp_out`. We will open `fp_in` and take the mean of the data along a single axis (`'x'`) and save it to `fp_out`. We're using `xarray` to do this here, so that it can be done over multiple cores in parallel.\n",
    "\n",
    "We also return an integer (1) when the routine has been successful just to show how CloudBatch_apply will concatenate outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd32733-077f-422d-9f21-3470553503f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_func(fp_in, fp_out):\n",
    "    ds = xr.open_dataset(fp_in, chunks={'lat':1e3, 'lon':1e3})\n",
    "    data_out = ds.mean(dim='lat')\n",
    "    data_out.to_netcdf(fp_out)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7177b3-bc0a-4b5f-b1fc-3747f3902137",
   "metadata": {},
   "source": [
    "Now we call `CloudBatch_apply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92225c2-069a-4104-9411-36ab135ffb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "app = CloudBatch_apply(func = batch_func, \n",
    "                    batch = [gsb_get, gsb_put],\n",
    "                    verbosity = 2,\n",
    "                    pass_args = 'one',\n",
    "                    delete_put_files = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
